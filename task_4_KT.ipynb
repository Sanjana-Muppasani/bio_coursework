{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1532479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import keras \n",
    "import tarfile\n",
    "import os\n",
    "import lzma\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Dense, Dropout, Flatten\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.backend import clear_session\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy import stats\n",
    "import keras_tuner as kt\n",
    "from keras.layers import GRU, SimpleRNN, Dense, Dropout, Embedding\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbe20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/cath.tar.xz\"\n",
    "output_dir = \"data/extracted_data/cath\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with lzma.open(filepath) as xz_file:\n",
    "    with tarfile.open(fileobj=xz_file) as tar:\n",
    "        tar.extractall(path=output_dir)\n",
    "\n",
    "FN_DOMAIN_LIST = 'data/extracted_data/cath/proteins/domain_classification.txt'\n",
    "FN_SF_NAMES = 'data/extracted_data/cath/proteins/superfamily_names.txt'\n",
    "FN_SEQ_S60 = 'data/extracted_data/cath/proteins/seqs_S60.fa'\n",
    "\n",
    "sequences = []\n",
    "current_id = None\n",
    "current_seq = []\n",
    "\n",
    "\n",
    "with open(FN_SEQ_S60, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith('>'):\n",
    "            if current_id:\n",
    "                sequences.append({\n",
    "                    'domain_id': current_id, \n",
    "                    'sequence': \"\".join(current_seq)\n",
    "                })\n",
    "        \n",
    "            parts = line[1:].split('|')\n",
    "            \n",
    "            if len(parts) >= 3:\n",
    "                full_code = parts[2]\n",
    "                current_id = full_code.split('/')[0]\n",
    "            else:\n",
    "                current_id = line[1:].split()[0]\n",
    "\n",
    "            current_seq = []\n",
    "        else:\n",
    "            current_seq.append(line)\n",
    "    \n",
    "    # Save last entry\n",
    "    if current_id:\n",
    "        sequences.append({'domain_id': current_id, 'sequence': \"\".join(current_seq)})\n",
    "            \n",
    "df_seq =  pd.DataFrame(sequences)\n",
    "\n",
    "\n",
    "col_names = ['domain_id', 'C', 'A', 'T', 'H', 'S', 'O', 'L', 'I', 'D', 'len', 'res']\n",
    "\n",
    "df_domains = pd.read_csv(\n",
    "    FN_DOMAIN_LIST, \n",
    "    sep=r'\\s+', \n",
    "    comment='#', \n",
    "    header=None,\n",
    "    names=col_names,\n",
    "    usecols=['domain_id', 'C', 'A', 'T', 'H','S', 'O', 'L', 'I', 'D', 'len', 'res']\n",
    ")\n",
    "\n",
    "# Create Superfamily ID (C.A.T.H)\n",
    "df_domains['superfamily_id'] = df_domains.apply(\n",
    "    lambda x: f\"{x['C']}.{x['A']}.{x['T']}.{x['H']}\", axis=1\n",
    ")\n",
    "\n",
    "sf_names = {}\n",
    "with open(FN_SF_NAMES, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('#'): continue\n",
    "        parts = line.strip().split(maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            sf_names[parts[0]] = parts[1]\n",
    "\n",
    "df_merged= pd.merge(df_seq, df_domains, on='domain_id', how='inner')\n",
    "\n",
    "sf_counts = df_merged['superfamily_id'].value_counts()\n",
    "small_sfs = sf_counts[sf_counts < 1000]\n",
    "top_5_sfs = small_sfs.nlargest(5).index.tolist()\n",
    "df_filtered = df_merged[df_merged['superfamily_id'].isin(top_5_sfs)].copy()\n",
    "\n",
    "final_data = df_merged[df_merged['superfamily_id'].isin(top_5_sfs)].astype(str).copy()\n",
    "final_data = final_data[[\"sequence\", \"superfamily_id\"]].reset_index(drop=True)\n",
    "\n",
    "max_len = int(final_data.sequence.str.len().max())\n",
    "\n",
    "vectorizer = keras.layers.TextVectorization(split=\"character\", output_sequence_length=max_len)\n",
    "vectorizer.adapt(final_data.sequence)\n",
    "x = vectorizer(final_data.sequence)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(final_data.superfamily_id)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "x_numpy = x.numpy() if hasattr(x, 'numpy') else np.array(x)\n",
    "y_numpy = y.numpy() if hasattr(y, 'numpy') else np.array(y)\n",
    "y_numpy = y_numpy.astype(int)\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1c528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CNN Tuner Search...\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "64                |64                |units\n",
      "0.5               |0.5               |dropout\n",
      "0.01              |0.01              |learning_rate\n",
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.4463 - loss: 1.3403 - val_accuracy: 0.4733 - val_loss: 1.1512\n",
      "Epoch 2/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5019 - loss: 1.1452"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# --- 3. RUN SEARCH ---\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting CNN Tuner Search...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mcnn_tuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     61\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# --- 4. GET RESULTS ---\u001b[39;00m\n\u001b[32m     64\u001b[39m best_hps = cnn_tuner.get_best_hyperparameters(num_trials=\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanja\\bio_coursework\\.venv\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[39m, in \u001b[36mBaseTuner.search\u001b[39m\u001b[34m(self, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_trial_begin(trial)\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_trial_end(trial)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.on_search_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanja\\bio_coursework\\.venv\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[39m, in \u001b[36mBaseTuner._try_run_and_update_trial\u001b[39m\u001b[34m(self, trial, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, *fit_args, **fit_kwargs):\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m         trial.status = trial_module.TrialStatus.COMPLETED\n\u001b[32m    276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanja\\bio_coursework\\.venv\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[39m, in \u001b[36mBaseTuner._run_and_update_trial\u001b[39m\u001b[34m(self, trial, *fit_args, **fit_kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, *fit_args, **fit_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[32m    241\u001b[39m         \u001b[38;5;28mself\u001b[39m.oracle.objective.name\n\u001b[32m    242\u001b[39m     ):\n\u001b[32m    243\u001b[39m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[32m    244\u001b[39m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[32m    245\u001b[39m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[32m    246\u001b[39m         warnings.warn(\n\u001b[32m    247\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe use case of calling \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    254\u001b[39m             stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    255\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanja\\bio_coursework\\.venv\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[39m, in \u001b[36mTuner.run_trial\u001b[39m\u001b[34m(self, trial, *args, **kwargs)\u001b[39m\n\u001b[32m    312\u001b[39m     callbacks.append(model_checkpoint)\n\u001b[32m    313\u001b[39m     copied_kwargs[\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m] = callbacks\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     obj_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m     histories.append(obj_value)\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanja\\bio_coursework\\.venv\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[39m, in \u001b[36mTuner._build_and_fit_model\u001b[39m\u001b[34m(self, trial, *args, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m hp = trial.hyperparameters\n\u001b[32m    232\u001b[39m model = \u001b[38;5;28mself\u001b[39m._try_build(hp)\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.config.multi_backend():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanja\\bio_coursework\\.venv\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[39m, in \u001b[36mHyperModel.fit\u001b[39m\u001b[34m(self, hp, model, *args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, *args, **kwargs):\n\u001b[32m    126\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[32m    127\u001b[39m \n\u001b[32m    128\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m \u001b[33;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanja\\bio_coursework\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanja\\bio_coursework\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:434\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    413\u001b[39m         \u001b[38;5;28mself\u001b[39m._eval_epoch_iterator = TFEpochIterator(\n\u001b[32m    414\u001b[39m             x=val_x,\n\u001b[32m    415\u001b[39m             y=val_y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    421\u001b[39m             shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    422\u001b[39m         )\n\u001b[32m    423\u001b[39m     val_logs = \u001b[38;5;28mself\u001b[39m.evaluate(\n\u001b[32m    424\u001b[39m         x=val_x,\n\u001b[32m    425\u001b[39m         y=val_y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    431\u001b[39m         _use_cached_eval_dataset=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    432\u001b[39m     )\n\u001b[32m    433\u001b[39m     val_logs = {\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m \u001b[43mval_logs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m     }\n\u001b[32m    436\u001b[39m     epoch_logs.update(val_logs)\n\u001b[32m    438\u001b[39m callbacks.on_epoch_end(epoch, epoch_logs)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "TOKENS = 24\n",
    "DIMENSIONS = 16\n",
    "CLASSES = 5\n",
    "SIZE = 4  # Kernel/Pool size constant\n",
    "\n",
    "# 0. DATA SPLIT (Matches your example)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_numpy, y_numpy, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# --- 1. DEFINE CNN MODEL ---\n",
    "def build_cnn(hp):\n",
    "    # Tunable parameters\n",
    "    units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "    dropout = hp.Choice('dropout', values=[0.2, 0.5])\n",
    "    lr = hp.Choice('learning_rate', values=[1e-2, 1e-3])\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=(max_len,)),\n",
    "        Embedding(TOKENS, DIMENSIONS, mask_zero=False), # mask_zero=False is better for CNNs\n",
    "\n",
    "        # CNN Layer: Uses tuned 'units' for filters\n",
    "        # We use padding='same' to ensure Flatten works safely\n",
    "        Conv1D(filters=units, kernel_size=SIZE, activation=\"relu\", padding='same'),\n",
    "        MaxPooling1D(pool_size=SIZE),\n",
    "        \n",
    "        Flatten(),\n",
    "\n",
    "        Dense(units, activation=\"relu\"),\n",
    "        Dropout(dropout),\n",
    "\n",
    "        Dense(CLASSES, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- 2. SETUP THE TUNER ---\n",
    "cnn_tuner = kt.RandomSearch(\n",
    "    build_cnn,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='cnn_tuning_v1', # Unique name\n",
    "    overwrite=True                # CRITICAL: Deletes old logs to prevent errors\n",
    ")\n",
    "\n",
    "# --- 3. RUN SEARCH ---\n",
    "print(\"Starting CNN Tuner Search...\")\n",
    "cnn_tuner.search(\n",
    "    x_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(x_val, y_val),\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 4. GET RESULTS ---\n",
    "best_hps = cnn_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "CNN Search Complete.\n",
    "Best units: {best_hps.get('units')}\n",
    "Best learning rate: {best_hps.get('learning_rate')}\n",
    "Best dropout: {best_hps.get('dropout')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a505785b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 18m 47s]\n",
      "val_accuracy: 0.5047169923782349\n",
      "\n",
      "Best val_accuracy So Far: 0.7924528121948242\n",
      "Total elapsed time: 01h 30m 49s\n",
      "\n",
      "--- Tuning Complete. Best Hyperparameters: ---\n",
      "Embedding Dimensions: 32\n",
      "Units: 32\n",
      "Dropout: 0.2\n",
      "Learning Rate: 0.01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- CONFIGURATION ---\n",
    "EPOCHS_TUNING = 50\n",
    "MAX_TRIALS = 5 \n",
    "TOKENS = 24       # Assumed from your previous message\n",
    "CLASSES = 5       # Assumed from your previous message\n",
    "\n",
    "# --- 1. DATA SPLIT FOR TUNING ---\n",
    "# (Assuming x and y are already defined)\n",
    "x_tune_train, x_tune_val, y_tune_train, y_tune_val = train_test_split(\n",
    "    x_numpy, y_numpy, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# --- 2. DEFINE THE HYPERMODEL (GRU + RNN) ---\n",
    "def build_gru_rnn(hp):\n",
    "    # --- Tunable Hyperparameters ---\n",
    "    \n",
    "    # 1. NEW: Tune Embedding Dimensions\n",
    "    # We try 8, 16, 32, and 64 to see how \"wide\" the vector needs to be.\n",
    "    embed_dim = hp.Choice('embedding_dim', values=[8, 16, 32, 64])\n",
    "\n",
    "    # 2. Tune Units (GRU)\n",
    "    units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "    \n",
    "    # 3. Tune Dropout\n",
    "    dropout = hp.Choice('dropout', values=[0.2, 0.5])\n",
    "    \n",
    "    # 4. Tune Learning Rate\n",
    "    lr = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=(max_len,)),\n",
    "        \n",
    "        # Pass the tuned 'embed_dim' here instead of a fixed constant\n",
    "        Embedding(input_dim=TOKENS, output_dim=embed_dim, mask_zero=True),\n",
    "\n",
    "        # Layer 1: GRU\n",
    "        GRU(units, return_sequences=True),\n",
    "        \n",
    "        # Layer 2: SimpleRNN (Half the size of the GRU)\n",
    "        SimpleRNN(units // 2),\n",
    "\n",
    "        Dense(units, activation=\"relu\"),\n",
    "        Dropout(dropout),\n",
    "\n",
    "        Dense(CLASSES, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- 3. PHASE 1: HYPERPARAMETER TUNING ---\n",
    "print(f\"--- Phase 1: Tuning Hyperparameters (Max Trials: {MAX_TRIALS}) ---\")\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_gru_rnn,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=MAX_TRIALS,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='gru_rnn_tuning_v2', # Changed name to avoid conflict with previous runs\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "tuner.search(\n",
    "    x_tune_train, y_tune_train,\n",
    "    epochs=EPOCHS_TUNING,\n",
    "    validation_data=(x_tune_val, y_tune_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"\\n--- Tuning Complete. Best Hyperparameters: ---\")\n",
    "print(f\"Embedding Dimensions: {best_hps.get('embedding_dim')}\")  # New print\n",
    "print(f\"Units: {best_hps.get('units')}\")\n",
    "print(f\"Dropout: {best_hps.get('dropout')}\")\n",
    "print(f\"Learning Rate: {best_hps.get('learning_rate')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_coursework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
